{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report\n",
    "In the course of this project, my wrangling skills were put to test by gathering, assessing and cleaning three (3) different kinds of datasets( .csv, .tsv and .txt) files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering\n",
    "The first step of data wrangling is Gathering. Gathering involves sourcing data from various sources or locations. \n",
    "Three datasets were gathered using 3 different means. The first dataset was a CSV file gathered using the pandas read_csv, the second dataset was a tsv using the requests library and the third dataset was gathered using the JSON library \n",
    "\n",
    "After all the datasets were gathered, they were all saved to their respective files and read into a data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing \n",
    "Assessing means visually or pragmatically checking the data for tidiness and quality issues.\n",
    "Visually assessing the 3 datasets involved viewing the data frames with the `.head()` method.\n",
    "This allows us to view the first 5 rows and all columns of the datasets.\n",
    "\n",
    "Pragmatically assessing the data using code. During this part of the wrangling process, several pandas methods were used in the assessment of the datasets. Examples of these methods include:\n",
    "`.info()`\n",
    "`.duplicated()`\n",
    "`.unique()`\n",
    "`.value_counts()`\n",
    "`.describe()`\n",
    "\n",
    "Other code used include:\n",
    "`.str.islower()`\n",
    "`.str.contains()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "Cleaning involves correcting all tidiness and quality issues found. This stage was done with the define-code-test process.  Examples of the issues found include:\n",
    "- Erroneous data types\n",
    "- Invalid and incomplete names\n",
    "- Unrelated columns, Etc\n",
    "\n",
    "Cleaning processes were: \n",
    "\n",
    "1. The correct data(tweet source) was extracted from the unclean source column\n",
    "2. All missing data were set to NaN\n",
    "3. Rating denominators greater than 10 were set to 10\n",
    "4. The timestamp column was converted to DateTime data type \n",
    "5. Irrelevant and unneeded columns were dropped\n",
    "6. Names were correctly extracted\n",
    "7. Dog stages were summarized in one column\n",
    "8. Retweet records were dropped\n",
    "9. Rows that said ‘We only rate dogs’ were dropped\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Data\n",
    "After the wrangling process, the data were merged and stored as a CSV file. Before storing as a CSV file, all duplicates were dropped.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "085e252e8eacd9eba7d41a3e5a03115f1c4cd82e6520e14d11e622873a1c979d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
